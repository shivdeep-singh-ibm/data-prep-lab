{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
   "metadata": {},
   "source": [
    "# Data processing Pipeline <a class=\"anchor\" id=\"top\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053ecf08-5f62-4b99-9347-8a0955843d21",
   "metadata": {},
   "source": [
    "Pipeline in this context is a name for a data processing pipeline which transforms data using a series of components called transforms(annotators/filters)\n",
    "\n",
    "\n",
    "\n",
    "The list of the components used in this pipeline are:\n",
    "\n",
    "- [Ingest2parquet](#item1)\n",
    "- [Exact Dedup](#item2)\n",
    "- [Doc_ID generation](#item3)\n",
    "- [Fuzzy Dedup](#item4)\n",
    "- [Programming Language Annotation](#item5)\n",
    "- [Code quality annotation](#item6)\n",
    "- [Filtering](#item7)\n",
    "- [Tokenization](#item8)\n",
    "\n",
    "The notebook executes the above transforms one after another to give a sense of\n",
    "how data processing can be done using data processing library.\n",
    "\n",
    "It is possible to change input/output paths and supply your own data and execute the pipeline. Each transform takes in a set of parameters which can also be tweaked as per requirement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d976e-cb4c-4469-af39-4b7ea507e9d8",
   "metadata": {},
   "source": [
    "### Import Common python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66178913-42b8-426b-a2e9-9587268fd05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.ray import TransformLauncher\n",
    "from data_processing.utils import ParamsUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f30ec-e2e2-42b3-93f6-8b83dc7a4011",
   "metadata": {},
   "source": [
    "## Sample Data For the pipeline\n",
    "\n",
    "This sample pipeline can be used to process data needed to train/finetune models for code. Hence the data we need before starting the \n",
    "pipeline can be code repositories from say github/gitlab/bitbucket etc.\n",
    "\n",
    "### Collecting Data\n",
    "\n",
    "Easiest way to get data is to download repositories from github in zip format. \n",
    "\n",
    "Here's how to download a GitHub repository in ZIP format:\n",
    "\n",
    "1. Go to the desired repository on GitHub.\n",
    "2. Click the \"Code\" button near the top right corner of the repository.\n",
    "3. Click the \"Download ZIP\" button.\n",
    "\n",
    "This will download a ZIP archive of the entire repository to your computer.\n",
    "\n",
    "Follow these steps and download some repositories from github into a folder. Now your data is ready.\n",
    "\n",
    "The folder containing this data would serve as the input to the pipeline. Assign the path of this data folder to the variable `zip_input_folder`. It would be used\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
   "metadata": {},
   "source": [
    "### Set input/output path variables for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac8bee-0960-4309-b225-d7a211b14262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# We can set input paths here\n",
    "zip_input_folder = <UPDATE PATH TO INPUT DATA HERE>\n",
    "\n",
    "\n",
    "if not os.path.exists(zip_input_folder):\n",
    "    print (\"NO INPUT DATA\")\n",
    "    print (\"Please set `zip_input_folder` variable to path containing data\")\n",
    "\n",
    "# make sure the paths are correct\n",
    "data_base_path = \"test-data\"\n",
    "\n",
    "parquet_data_output = os.path.join(data_base_path, \"parquet_input\")\n",
    "\n",
    "ededup_out =  os.path.join(data_base_path, \"ededup_out\")\n",
    "\n",
    "doc_id_out =  os.path.join(data_base_path, \"doc_id_out\")\n",
    "fdedup_out = os.path.join(data_base_path, \"fdedup_out\")\n",
    "\n",
    "lang_out =  os.path.join(data_base_path,\"lang_out\")\n",
    "cq_out = os.path.join(data_base_path,\"cq_out\")\n",
    "\n",
    "filter_out = os.path.join(data_base_path ,\"filter_out\")\n",
    "tokensization_out = os.path.join(data_base_path ,\"tokenization_out\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\"> 1. Convert data to parquet using ingest2parquet [<-](#top)<a class=\"anchor\" id=\"item1\"></a>\n",
    "_zip_ to _parquet_ </span>\n",
    "\n",
    "Raw code data files which are in zip format are converted to parquet files, where each row of the parquet file corresponds to a separate code file. Apart from the contents of the code file, every row also contains a unique document id, file URL, name of the repository, source of the data, date of acquisition and license of the repository. For every code file, a language field is also added, which is detected using the filename\n",
    "extensions.\n",
    "\n",
    "It is advised to prepare a dataset in the folder `test-data/input`. The dataset should contain zip\n",
    "files of github repos. One way to make this dataset is to download github repos in zip format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482605b2-d814-456d-9195-49a2ec454ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this stage input folder contains the zip files, each zip file contains a github repo.\n",
    "\n",
    "input_folder = zip_input_folder\n",
    "output_folder =  parquet_data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from data_processing.utils import ParamsUtils\n",
    "import sys\n",
    "import ast\n",
    "from ingest2parquet import ingest2parquet\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "params = {\n",
    "        \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "        \"data_files_to_use\": ast.literal_eval(\"['.zip']\"),\n",
    "        \"detect_programming_lang\": True,\n",
    "        \"snapshot\": \"github\",\n",
    "        \"domain\": \"code\"\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# Launch\n",
    "\n",
    "ingest2parquet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
   "metadata": {},
   "source": [
    "##  <span style=\"color: green\">   2. Exact Dedup [<-](#top)<a class=\"anchor\" id=\"item2\"></a> </span>\n",
    "\n",
    "Remove documents having identical code to remove bias in the training data. On the content of each document, a SHA256 hash is computed,\n",
    "followed by de-duplication of record having identical hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a0e1f6-ff53-40aa-96b1-096ade4bd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For this stage the input is the folder containing parquet data which is output from the ingest2parquet tool\n",
    "\n",
    "input_folder = parquet_data_output\n",
    "output_folder = ededup_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import ededup transform configuration\n",
    "from ededup_transform import EdedupTableTransformConfiguration\n",
    "\n",
    "\n",
    "# Prepare the commandline params\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # ededup parameters\n",
    "    \"ededup_hash_cpu\": 0.5,\n",
    "    \"ededup_num_hashes\": 2,\n",
    "    \"ededup_doc_column\": \"contents\",\n",
    "}\n",
    "\n",
    "# Pass the commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "# create launcher\n",
    "ededup_launcher = TransformLauncher(transform_runtime_config=EdedupTableTransformConfiguration())\n",
    "ededup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f4d00-33bb-4d9a-9f34-4d7f3ee0b7bc",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  3. DOC ID generation [<-](#top)<a class=\"anchor\" id=\"item3\"></a> </span>\n",
    "\n",
    "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
    "\n",
    " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set hash_column to the name of the column, where you want to store it.\n",
    " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set int_id_column to the name of the column, where you want to store it. **This is needed is we want to run fuzzy dedup** in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f62394-fbde-495c-bbbb-83161b006bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for this stage is the output of exact dedeup component\n",
    "# output of this component makes it possible for fdedup component to run on data.\n",
    "\n",
    "input_folder = ededup_out\n",
    "output_folder = doc_id_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daf36d-686c-4e0a-aabf-ce55f999bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doc_id_transform import DocIDTransformConfiguration\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # doc id configuration\n",
    "    \"doc_id_doc_column\": \"contents\",\n",
    "    \"doc_id_hash_column\": \"hash_column\",\n",
    "    \"doc_id_int_column\": \"int_id_column\",\n",
    "}\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "launcher = TransformLauncher(transform_runtime_config=DocIDTransformConfiguration())\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85309751-8556-41c6-ac32-84acc941bc8d",
   "metadata": {},
   "source": [
    "## 4. <span style=\"color: green\">  Fuzzy Dedup [<-](#top)<a class=\"anchor\" id=\"item4\"></a> </span>\n",
    "\n",
    "Post exact deduplication, fuzzy deduplication is applied with\n",
    "the goal of removing code files that may have slight variations and thereby unbiasing\n",
    "the data further. Small variations are quite commonly seen in code data in the form\n",
    "of variations in the values of variables, addittion of logging statements etc. Find near-\n",
    "duplicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input to this component is the output of doc_id generator component. \n",
    "\n",
    "input_folder = doc_id_out\n",
    "output_folder = fdedup_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.ray import TransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "from fdedup_transform import FdedupTableTransformConfiguration\n",
    "\n",
    "# create parameters\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # Orchestration parameters\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    # columns used\n",
    "    \"fdedup_doc_column\": \"contents\",\n",
    "    \"fdedup_id_column\": \"int_id_column\",\n",
    "    \"fdedup_cluster_column\": \"hash_column\",\n",
    "    # infrastructure\n",
    "    \"fdedup_bucket_cpu\": 0.5,\n",
    "    \"fdedup_doc_cpu\": 0.5,\n",
    "    \"fdedup_mhash_cpu\": 0.5,\n",
    "    \"fdedup_num_doc_actors\": 2,\n",
    "    \"fdedup_num_bucket_actors\": 1,\n",
    "    \"fdedup_num_minhash_actors\": 1,\n",
    "    \"fdedup_num_preprocessors\": 2,\n",
    "    # fuzzy parameters\n",
    "    \"fdedup_num_permutations\": 64,\n",
    "    \"fdedup_threshold\": 0.8,\n",
    "    \"fdedup_shingles_size\": 5,\n",
    "    \"fdedup_delimiters\": \" \"\n",
    "}\n",
    "\n",
    "# Pass commandline params\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "\n",
    "fdedup_launcher = TransformLauncher(transform_runtime_config=FdedupTableTransformConfiguration())\n",
    "fdedup_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1d761-62a7-4a12-ad23-b2c268ad8ed2",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  5. Programming language annotation [<-](#top)<a class=\"anchor\" id=\"item5\"></a> </span>\n",
    "\n",
    "The raw data may contains many programming languages. Of this, we would wish to retain a prioritised list of selected programming languages. This component takes a file which has new line separated names of languages we need to select. It annotates the data a new column with boolean values. This column can be used by filter component to select the required languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db05e1e-4c62-4367-93ca-b2ddff95e4b4",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ec4fb6-fa62-45d1-9aa1-596d7182b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_folder = fdedup_out\n",
    "output_folder = lang_out \n",
    "selected_languages_file = \"./test-data/allowed-code-languages.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e7e5a-064f-4dca-a017-4211f7a3e980",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbb2a3-a6f4-4a3d-bb2f-8491fd063611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from data_processing.ray import TransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "from proglang_select_transform import (\n",
    "    ProgLangSelectTransformConfiguration,\n",
    "    lang_allowed_langs_file_key,\n",
    "    lang_lang_column_key,\n",
    "    lang_output_column_key,\n",
    ")\n",
    "\n",
    "# create parameters\n",
    "language_column_name = \"programming_language\"\n",
    "annotated_column_name = \"lang_selected\"\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "langselect_config = {\n",
    "    lang_allowed_langs_file_key: selected_languages_file,\n",
    "    lang_lang_column_key: language_column_name,\n",
    "    lang_output_column_key: annotated_column_name,\n",
    "}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 1,\n",
    "    # language selection specific parameters\n",
    "    **langselect_config,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# create launcher\n",
    "launcher = TransformLauncher(transform_runtime_config=ProgLangSelectTransformConfiguration())\n",
    "launcher.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646cbb7-3046-44c0-827d-d102d3ff7cb8",
   "metadata": {},
   "source": [
    "## <span style=\"color: green\">  6. Code Quality [<-](#top)<a class=\"anchor\" id=\"item6\"></a> </span>\n",
    "\n",
    "We experiment with various code quality metrics but finally retain\n",
    "the four code quality metrics used by (Li et al., 2023) to balance the tradeoff between\n",
    "code quality versus data volume. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e985668-848b-4633-b0d8-9fe70ada0c91",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f080011-c9fe-430e-9ecc-f2220d2c8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = lang_out\n",
    "output_folder = cq_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02982c5-f398-4a1a-a9fe-42d7ae748c7c",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29319fb9-b0d8-4f86-9bc5-b92960ad8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from code_quality_transform import CodeQualityTransformConfiguration\n",
    "from data_processing.ray import TransformLauncher\n",
    "from data_processing.utils import ParamsUtils\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "language_column_name = \"programming_language\"\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 3,\n",
    "    \"runtime_pipeline_id\": \"pipeline_id\",\n",
    "    \"runtime_job_id\": \"job_id\",\n",
    "    \"runtime_creation_delay\": 0,\n",
    "    \"runtime_code_location\": ParamsUtils.convert_to_ast(code_location),\n",
    "    # code quality configuration\n",
    "    \"cq_contents_column_name\": \"contents\",\n",
    "    \"cq_language_column_name\": language_column_name,\n",
    "}\n",
    "\n",
    "\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "\n",
    "# launch\n",
    "# create launcher\n",
    "launcher = TransformLauncher(transform_runtime_config=CodeQualityTransformConfiguration())\n",
    "\n",
    "\n",
    "cq_launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec9839-13b4-4d32-89c5-38f59c5a89f0",
   "metadata": {},
   "source": [
    "## 7. <span style=\"color: green\">   Filtering [<-](#top)<a class=\"anchor\" id=\"item7\"></a> </span>\n",
    "\n",
    "Filter out documents that do not meet the quality threshold for each annotation. The thresholds are computed based on a distributional\n",
    "analysis as well as manual inspection of samples maintaining the balance between data quality and data volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c54d69-8aee-4f0f-b74c-35dc0609270f",
   "metadata": {},
   "source": [
    "### Set Input/output Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7991811-b19e-43b5-89ac-b24060c0ccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = cq_out\n",
    "output_folder = filter_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c460e05c-aeee-4b53-9dd5-8dfa1afc0ece",
   "metadata": {},
   "source": [
    "### Execute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dea2b0-0e54-4912-8620-886e2b8420ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from data_processing.data_access import DataAccessLocal\n",
    "from filter_transform import (\n",
    "    FilterTransformConfiguration,\n",
    "    filter_columns_to_drop_cli_param,\n",
    "    filter_criteria_cli_param,\n",
    "    filter_logical_operator_cli_param,\n",
    ")\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "\n",
    "# This is just an example criteria to filter\n",
    "filter_criteria = [\n",
    "    \"total_num_lines > 10 AND total_num_lines < 90\",\n",
    "    \"lang_selected = 1\",\n",
    "]\n",
    "filter_logical_operator = \"AND\"\n",
    "filter_columns_to_drop = [\"lang_selected\", \"hash_column\"]\n",
    "\n",
    "filter_params = {\n",
    "    filter_criteria_cli_param: filter_criteria,\n",
    "    filter_columns_to_drop_cli_param: filter_columns_to_drop,\n",
    "    filter_logical_operator_cli_param: filter_logical_operator,\n",
    "}\n",
    "\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "launcher_params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(launcher_params | filter_params)\n",
    "    # Create the longer to launch with the blocklist transform.\n",
    "launcher = TransformLauncher(transform_runtime_config=FilterTransformConfiguration())\n",
    "    # Launch the ray actor(s) to process the input\n",
    "launcher.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
   "metadata": {},
   "source": [
    "## 8. <span style=\"color: green\">  Tokenization [<-](#top)<a class=\"anchor\" id=\"item8\"></a> </span>\n",
    "\n",
    "The data tokenization transform maps a (non-empty) input table to an output table using a pre-trained tokenizer. The input table must contain at least two columns, by default named document_id and contents. The tokenization transform utilizes the pre-trained tokenizer to tokenize each row (assuming a document) in the input table to each row in the output folder.\n",
    "\n",
    "A pre-trained tokenizer must be specified through the --tkn_tokenizer parameter, which can be the name of a ready-for-download tokenizer from HuggingFace such as hf-internal-testing/llama-tokenizer, bigcode/starcoder or any others that can loaded by the Huggingface AutoTokenizer library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = filter_out\n",
    "output_folder = tokensization_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228df6b2-bc62-494b-9697-03ece98d7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenization_transform import TokenizationTransformConfiguration\n",
    "\n",
    "local_conf = {\n",
    "    \"input_folder\": input_folder,\n",
    "    \"output_folder\": output_folder,\n",
    "}\n",
    "worker_options = {\"num_cpus\": 0.8}\n",
    "params = {\n",
    "    # where to run\n",
    "    \"run_locally\": True,\n",
    "    # Data access. Only required parameters are specified\n",
    "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
    "    # orchestrator\n",
    "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
    "    \"runtime_num_workers\": 5,\n",
    "}\n",
    "\n",
    "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
    "# create launcher\n",
    "launcher = TransformLauncher(transform_runtime_config=TokenizationTransformConfiguration())\n",
    "# Launch the ray actor(s) to process the input\n",
    "launcher.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
